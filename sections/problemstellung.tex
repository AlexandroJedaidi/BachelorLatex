\section{Theorie gewöhnlicher Differentialgleichungen}
\label{sec:theorie}
In diesem Abschnitt wird die Theorie zu gewöhnlicher Differentialgleichungen erläutert. Diese orientiert sich
hauptsächlich an den Arbeiten \cite{peterdeuflhardfolkmarbornemannNumerischeMathematikGewohnliche},
\cite{ernsthairergerhardwannerSolvingOrdinaryDifferential} und \cite{berndaulbachGewohnlicheDifferentialgleichungen2004}.
Folgendes ist Grundlage für das Verständnis der Schlussfolgerungen und Ergebnisse dieser Arbeit.

\subsection{Gewöhnliche Differentialgleichungen und Anfangswertprobleme}
Wir beginnen mit der allgemeinen Definition einer gewöhnlichen Differentialgleichung erster Ordnung.
\begin{definition}
    Ein System {\em gewöhnlicher Differentialgleichungen erster Ordnung} hat die Form
    \begin{align}
        x^{\prime} = f(t, x, x^{\prime}, x^{\prime\prime}, \dots, x^{(m-1)}) \label{eq:1}
    \end{align}
    mit der gegebenen Funktion
    $
    f : D \times \mathbb{R}^{n} \rightarrow \mathbb{R}^{n},
    $
    wobei $D \subseteq \mathbb{R}$ ein Zeitintervall ist. Eine dazugehörige {\em Lösung} (falls existent)
    $\hat{x} : D \rightarrow \mathbb{R}^n$ ist eine stetig, differenzierbare Funktion welche die Bedingung
    \[
        \hat{x}^{\prime} = f(t, \hat{x}).
    \]
    erfüllt.
\end{definition}
\begin{definition}
    Ein {\em Anfangswertproblem} für eine Differentialgleichung \eqref{eq:1} mit gegebenen Anfangswerten
    $x_{0} \in \mathbb{R}^{n}$ und der Anfangszeit $t_0 \in D \subset \mathbb{R}$ hat die Form
    \begin{align}
        x^{\prime} = f(t, x),\quad x(t_{0})=x_{0}. \label{eq:2}
    \end{align}
\end{definition}
Eine Lösung des Problems $\hat{x} : D \rightarrow \mathbb{R}$ muss also zusätzlich zu \eqref{eq:1} auch die
Anfangswertbedingungen\eqref{eq:2} erfüllen.
\begin{bem}
    Neben Differentialgleichungen erster Ordnung existieren auch gewöhnliche Differentialgleichungen $m$-ter Ordnung.
    Diese haben die Form
    \begin{align}
        x^{(m)} = f(t, x, x^{\prime}, x^{\prime\prime}, \dots, x^{(m-1)}),
    \end{align}
    $
    f : D \times \mathbb{R}^{n} \times \mathbb{R}^{n} \times \dots \times \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}
    $
    und $D \subseteq \mathbb{R}$ ein Zeitintervall ist. Eine dazugehörige \textit{Lösung} (falls existent)
    $\hat{x} : D \rightarrow \mathbb{R}^n$ ist eine $m-mal$ stetig, differenzierbare Funktion welche die Bedingung
    \[
        \hat{x}^{(m)} = f(t, \hat{x},\hat{x}^{\prime},\hat{x}^{\prime\prime}, \dots,\hat{x}^{(m-1)}).
    \]
    erfüllt. Das zugehörige Anfangswertproblem $m$-ter Ordnung mit gegebenen Anfangswerten
    $x_{0,1},\dots,x_{0,m} \in \mathbb{R}^{n}$ und der Anfangszeit $t_0 \in D \subset \mathbb{R}$ erfüllt zusätzlich die
    Bedingungen \[
        x(t_{0})=x_{0,1},\quad x^{\prime}(t_{0})=x_{0,2},\dots,x^{(m-1)}(t_{0})=x_{0,m}.
    \]
    Außerdem ist es möglich jede gewöhnliche Differentialgleichung $m$-ter Ordnung zu einem System gewöhnlicher
    Differentialgleichungen $erster$ Ordnung umzuwandeln. Diese ist mit Hilfe der Funktionen
    $x_{j}:D \rightarrow \mathbb{R}$ für $j = 1,\dots,m$ äquivalent zu einem System erster Ordnung mit $m$ Gleichungen:
    \begin{align}
        x_{1}^{\prime}&=x_{2}, \nonumber \\
        x_{2}^{\prime}&=x_{3}, \nonumber \\
        &\vdots\\
        x_{m-1}^{\prime}&=x_{m}, \nonumber \\
        x_{m}^{\prime}&=f(t, x_{1}, x_{2}, x_{3}, \ldots, x_{m}). \nonumber \\ \nonumber
    \end{align}
    Für ein AWP \eqref{eq:2} gilt zusätzlich:
    \begin{align}
        x_{1}(t_{0})&=x_{0,1}, \nonumber \\
        x_{2}(t_{0})&=x_{0,2}, \nonumber \\
        &\vdots \\
        x_{m-1}(t_{0})&=x_{0,m-1}, \nonumber \\
        x_{m}(t_{0})&=x_{0,m}. \nonumber \\ \nonumber
    \end{align}
    Deshalb werden wir uns in dieser Arbeit ohne Beschränkung der Allgemeinheit auf gewöhnliche Differentialgleichungen
    erster Ordnung beschränken.
\end{bem}
\begin{definition}
    Eine gewöhnliche Differentialgleichung \eqref{eq:1} heißt {\em autonom}, wenn die rechte Seite $f$ nicht
    explizit von der unabhängigen Variable $t$ abhängt. Das bedeutet, es gilt
    \[
        x^{\prime} = f(x)
    \]
    auf ganz $D$.
\end{definition}
In ähnlichem Stil der Ordnungsreduktion können wir auch eine nichtautonome Differentialgleichung zu einem autonomen
System gewöhnlicher Differentialgleichungen umwandeln. Dazu führen wir eine neue Variable $z$ ein, für die gilt
\[
    z^{\prime} = 1, \quad z(t_0) = t_0.
\]
Dann ist das Anfangswertproblem \eqref{eq:2} äquivalent zu dem autonomen Anfangswertproblem zweiter Ordnung
\begin{alignat*}{3}
    z^{\prime} &= 1, \qquad &z(t_0) &= t_0 \\
    x^{\prime} &= f(z,x),\qquad  &x(t_0) &= x_0. \\
\end{alignat*}
Die Lösung von dem Anfangswertproblem \eqref{eq:2} hat also die Form $\left[ \begin{matrix}
                                         t\\
                                         x(t)\\
\end{matrix} \right].$
Weiterhin definieren wir in diesem Abschnitt noch eine Sonderform einer gewöhnlichen Differentialgleichung.
\begin{definition}
    Ein System gewöhnlicher Differentialgleichungen
    \begin{align}
        \label{linear-first-order-ode}
        x^{\prime}(t) = A(t)x(t) + g(t)
    \end{align}
    mit $A:D \rightarrow \mathbb{R}^{n \times n}$ und $g:D \rightarrow \mathbb{R}^n$ heißt
        {\em lineares System gewöhnlicher Differentialgleichungen}. Falls $g \equiv 0$, so nennt man
    \eqref{linear-first-order-ode} homogen, sonst inhomogen.
\end{definition}
Wir definieren nun steife lineare Differentialgleichungen unter der Bedingung dass die Koeffizientenmatrix $A$ konstant,
also unabhängig von $t$ ist.
\begin{definition}
    \label{steife-dgl}
    Eine lineare Differentialgleichung \eqref{linear-first-order-ode} mit $A \in \mathbb{R}^{n \times n}$ und
    $g(t) \in \mathbb{R}^n$ heißt $steif$, wenn
    \begin{align}
        1.& \quad Re(\lambda_j(A)) < 0, \qquad \text{ für alle Eigenwerte } \lambda_j(A) \text{ von A, }\\
        2.& \quad \min\limits_{1 \leq j \leq n} Re(\lambda_j(A)) \ll \max\limits_{1 \leq j \leq n} Re(\lambda_j(A)).
    \end{align}
\end{definition}
Wir werden später ein Verfahren betrachten, welches sich speziell dafür eignet, eine steife gewöhnliche
Differentialgleichung zu lösen.

\subsection{Existenz und Eindeutigkeit}
In diesem Abschnitt betrachten wir ein Anfangswertproblem $erster$ Ordnung
\begin{align}
    x^{\prime}&=f(t,x) \nonumber \\
    x(t_{0})&=x_{0} \label{firstorder-ode}
\end{align}
und zeigen, unter welchen Bedingungen der rechten Seite $f(t,x(t))$ eine Lösung existiert und ggf. eindeutig ist.

\subsubsection{Existenz von Lösungen}
Hier beweisen wir einen Satz, welcher zeigt, dass die Stetigkeit der rechten Seite $f$ für die Existenz einer
Lösung ausreicht. Dazu benötigen wir noch einen Satz aus der Funktionalanalysis.
\begin{satz}[Fixpunktsatz von Schauder, 2. Version]
    \label{fixpunktsatzvonschauder}
    Sei M eine nichtleere, abgeschlossene und konvexe Teilmenge eines Banachraums $X$ und $A:M \rightarrow M$ stetig.
    Dann besitzt $A$ wenigstens einen Fixpunkt $x$, sofern die Bildmenge $A(M)$ relativ kompakt ist.
\end{satz}
$Beweis.$ \cite[13,14]{sundermeierFixpunktsatzSchauder}\\
In dieser Arbeit wird mit $\left\lVert \cdot \right\rVert_2$ die euklidische Norm
$\left\lVert x \right\rVert_2 = \left( \sum_{i=1}^{n} (x_i)^2  \right)^{\frac{1}{2}}$ für $x \in \mathbb{R}^n$
beschreiben. Der Beweis des folgenden Satzes beruht auf das Buch \cite{harroheuserGewohnlicheDifferentialgleichungen}.
\begin{satz}[Existenzsatz von Peano]
\label{peano}
    Seien $t_0 \in \mathbb{R}$, $x_0 \in \mathbb{R}^n$,
    \[
        {\cal G}=\{(t,x) \in \mathbb{R} \times \mathbb{R}^{n}: |t-t_{0}| \leq \alpha, \quad
        \left\lVert x-x_{0} \right\rVert_2 \leq\beta, \quad \alpha,\beta > 0 \}
    \]
    und $f:{\cal G} \rightarrow \mathbb{R}^{n}$ stetig. Dann besitzt das Anfangswertproblem \eqref{firstorder-ode}
    mindestens eine Lösung $\hat{x}$ auf dem Intervall $D=(t_{0}-a,t_{0}+a)$, wobei
    \[
        a=\min\{\alpha, \frac{\beta}{M}\}, \qquad M= \max_{(t,y)\in {\cal G}}\left\lVert f(t,x)\right\rVert_2.
    \]
\end{satz}
$Beweis.$ Sei $J\subset D$ abgeschlossen mit $t_0 \in J$ und $\hat{x}(t)$ eine Lösung von \eqref{firstorder-ode}.
Da die rechte Seite $f$ auf ${\cal G}$ stetig ist, gilt nach dem Hauptsatz der Differential- und Integralrechnung:
\begin{align}
    \hat{x}(t) = \hat{x}_0 + \int_{t_0}^{t} f(s, \hat{x}(s)) ds \quad \text{für alle} \quad t \in J.
    \label{peano-proof}
\end{align}
Außerdem gilt für jede auf $J$ stetige Funktion $\hat{x}(t)$, die obige Gleichung erfüllt, dass sie auf $J$
differenzierbar ist. Dies bedeutet, dass eine auf $J$ stetige Funktion $\hat{x}(t)$ löst genau dann das
Anfangswertproblem \eqref{firstorder-ode}, wenn es die obige Integralgleichung erfüllt. Für eine auf $J$ stetige
Funktion $\hat{x}(t)$ gilt also:
\[
    \hat{x}(t) \text{ löst das AWP \eqref{firstorder-ode}}  \Leftrightarrow \hat{x}(t)
    \text{ erfüllt die Gleichung} \eqref{peano-proof}.
\]
Betrachte nun die Menge $K := \{ x \in C(D): \left\lVert x(t) - x_0 \right\rVert_2 \leq \beta \text{ für alle }
t \in D \}$, wobei $C(D)=\{g: g(t) \text{ ist stetig für alle } t \in D \}$. Die Menge K ist offensichtlich nicht leer
(betrachte bspw. die konstante Funktion $x \equiv x_0$). Jedem $x \in K$ wird nun eine Funktion $Ax \in C(D)$ zugewiesen
mit der Definition
\[
    (Ax)(t) := x_0 + \int_{t_0}^{t} f(s, x(s)) ds \quad \text{ für alle } \quad t \in D.
\]
Also gilt für $(Ax)(t)$ die Gleichung \eqref{peano-proof} und somit löst $Ax$ nach obiger Äquivalenz das
Anfangswertproblem auf $D$. Außerdem gilt:
\begin{align*}
    \left\lVert (Ax)(t) - x_0 \right\rVert_2 &= \left\lVert \int_{t_0}^{t} f(s, x(s)) ds \right\rVert_2 \\
    &\leq \int_{t_0}^{t} \left\lVert f(s, x(s)) \right\rVert_2 ds \\
    &\leq |t - t_0| \max_{(t,y)\in {\cal G}}\left\lVert f(t,x)\right\rVert_2 \\
    &\leq |t_0 + a - t_0| M \\
    &\leq \frac{\beta}{M}M = \beta \quad \text{ für alle } t \in D.
\end{align*}
%Für $A$ wird hier jedoch die Operatornorm
%$\left\lVert A \right\rVert = \sup\limits_{\left\lVert x \right\rVert = 1} \left\lVert Ax \right\rVert$ benutzt.
Das bedeutet, dass $Ax \in K$ für $x \in K$, bzw. $K \subset AK$.
\\!!TODO: $AK \subset K $!!\\
Nun können wir die hergeleitete Äquivalenz umformulieren: Für eine Funktion $\hat{x} \in K$ gilt
\[
    \hat{x} \text{ löst auf $D$ das Anfangswertproblem } \eqref{firstorder-ode} \Leftrightarrow \hat{x}
    \text{ ist ein Fixpunkt der Abbildung } A:K \rightarrow K.
\]
Dazu nutzen wir den Fixpunktsatz von Schauder \eqref{fixpunktsatzvonschauder}. Betrachte den Banachraum $C(D)$ mit der
Maximumsnorm
\[
    \left\lVert x \right\rVert_{\infty}:=\max_{t \in D} \left\lVert x(t) \right\rVert_2.
\]
Sei $(x_k)_{k \in \mathbb{N}} \subset K$ eine konvergente Folge mit einem Grenzwert $x \in C(D)$. Dann ist
\[
    \left\lVert x - x_0 \right\rVert_2 = \left\lVert \lim_{k \rightarrow \infty}(x_k) - x_0 \right\rVert_2
    = \lim_{k \rightarrow \infty} \left\lVert (x_k) - x_0 \right\rVert_2 \leq \lim_{k \rightarrow \infty} \beta = \beta.
\]
Also ist $x \in K$ und somit ist K abgeschlossen. Sei $x,y \in K $ beliebig und betrachte $v:=\lambda x + (1-\lambda)y$
mit $0 \leq \lambda \leq 1$. Dann ist $v \in K$, da
\begin{align*}
    \left\lVert v(t) - x_0 \right\rVert_2
    &= \left\lVert \lambda x(t) + (1-\lambda)y(t) - \lambda x_0 - (1-\lambda)y_0 \right\rVert_2 \\
    &= \lambda \left\lVert x(t) - x_0 \right\rVert_2 + (1 - \lambda) \left\lVert y(t) - y_0 \right\rVert_2 \\
    &\leq \beta (\lambda + 1 - \lambda) = \beta,
\end{align*}
für alle $t \in D$.
Das heißt, K ist außerdem konvex. Die Stetigkeit der Abbildung $A:K \rightarrow K$ lässt sich wie folgt zeigen.
Wir können ein $\delta > 0$ so bestimmen, sodass mit $\left\lVert x - y \right\rVert_2 < \delta$ gilt:
\[
    \left\lVert f(t,x) - f(t,y) \right\rVert_2 < \frac{\epsilon}{a}, \quad t \in D.
\]
Somit ist f gleichmäßig stetig.
Betrachte nun $x, y \in K$ mit $\left\lVert x - y \right\rVert_{\infty}$ also auch $\left\lVert x(t) - y(t) \right\rVert_2$
für alle $t \in D$, so ist für $t \in D$ immer $\left\lVert f(t,x(t)) - f(t,y(t)) \right\rVert_2 < \frac{\epsilon}{a}$ und
es folgt
\[
    \left\lVert (Ax)(t) - (Ay)(t) \right\rVert_2 \leq |t - t_0| \frac{\epsilon}{a} = \epsilon \text{ für alle } t \in D,
\]
also auch $\left\lVert Ax - Ay \right\rVert_{\infty} \leq \epsilon$. Es bleibt jetzt nur noch zu zeigen, dass die
Bildmenge $AK$ relativ kompakt ist. Sei hierfür $x \in K$ beliebig, dann ist
\[
    \left\lVert (Ax)(t) \right\rVert_2 \leq \left\lVert x_0 \right\rVert_2 + aM \quad \text{ für jedes } \quad t \in D
\] und
\[
    \left\lVert (Ax)(t_1) - (Ax)(t_2) \right\rVert_2 \leq |t_1 - t_2| M \quad \text{ für alle } \quad t_1,t_2 \in D.
\]
Somit ist $A(K)$ punktweise beschränkt und gleichgradig stetig auf dem kompakten Intervall $D$. Nach dem Satz von
Arzela-Ascoli \cite[49]{beckGewohnlicheDifferentialgleichungen2018} hat also jede Folge $(Ax)_n\subset AK$ eine
gleichmäßig konvergente Teilfolge. Dies gilt offensichtlich auch für die Maximimsnorm, wodurch $AK$ relativ kompakt
ist. \qedwhite\\

\subsubsection{Eindeutigkeit von Lösungen}
Ähnlich wie im vorherigen Abschnitt existiert ein Satz, welcher zeigt, dass eine im zweiten Argument $Lipschitz$-stetige
rechte Seite $f$ ausreicht, damit eine eindeutige Lösung für ein Anfangswertproblem \eqref{eq:2} existiert. Dazu
definieren wir zuerst eine Lipschitz-stetige Funktion.
\begin{definition}
    Sei $\mathcal{M} \subset \mathbb{R} \times \mathbb{R}^{n}$. Eine Funktion $f: \mathcal{M} \rightarrow \mathbb{R}^n$
    heißt genau dann Lipschitz-stetig auf $\mathcal{M}$ bezüglich des zweiten Arguments, wenn ein $L>0$ existiert,
    sodass gilt:
    \[
        \left\lVert f(t,x) - f(t,y) \right\rVert_2 \leq L \left\lVert x - y  \right\rVert_2
    \]
    für alle $(t,x), (t,y) \in \mathcal{M}$.
\end{definition}
Außerdem benötigen wir für den Beweis noch den Fixpunktsatz von Weissinger aus
\cite{harroheuserGewohnlicheDifferentialgleichungen}.
\begin{satz}[Fixpunktsatz von Weissinger]
    \label{Fixpunktsatzvonweissinger}
    Sei $\left( B, \left\lVert . \right\rVert \right) $ ein Banachraum und $U \subset B$ abgeschlossen und nichtleer.
    Ferner ist $\sum_{j=1}^{\infty} \alpha_j$ eine konvergente Reihe positiver Zahlen, also
    $\sum_{j=1}^{\infty} \alpha_j < \infty $, $\alpha_j \geq 0$, sowie $A:U \rightarrow U$ eine Selbstabbildung, für die
    \[
        \left\lVert A^{j}u - A^{j}v \right\rVert \leq \alpha_j \left\lVert u - v \right\rVert \quad \text{ für alle }
        \quad u,v \in U \quad \text{ und } \quad j \in \mathbb{N}
    \]
    gilt. Dann besitzt $A$ genau einen Fixpunkt in $U$, nämlich
    \[
        u = \lim_{n\rightarrow \infty} A^{n}u_0
    \]
    mit beliebigem $u_0 \in U$. Außerdem ist $u$ der Grenzwert der rekursiven Folge $u_j:=Au_{j-1}$ für $j \geq 1$
    mit $u_0 \in U$ beliebig und es gilt die Fehlerabschätzung
    \[
        \left\lVert u - u_n \right\rVert \leq \sum_{j=n}^{\infty} \alpha_j \left\lVert u_1 - u_0 \right\rVert.
    \]
\end{satz}
$Beweis.$ \cite[139]{harroheuserGewohnlicheDifferentialgleichungen}\\
%Es gilt
%\[
%    \left\lVert u_{j+1}-u_j \right\rVert_2 = \left\lVert A^j u_1 - A^j u_0 \right\rVert_2 \leq
%    \alpha_j \left\lVert u_1 - u_0 \right\rVert_2
%\] also können wir folgern
%\[
%    \left\lVert u_{j+k} - u_j \right\rVert_2 \leq \left\lVert u_{j+k-1} - u_{j+k-2} \right\rVert_2 + \dots +
%    \left\lVert u_{j+1} - u_j \right\rVert_2
%    \leq \underbrace{\left( \alpha_{j+k-1} + \dots + \alpha_{j} \right)}_{\rightarrow 0 \text{ für } j,k
%    \rightarrow\infty} \left\lVert u_1 - u_0\right\rVert_2
%\]
Nun können wir einen grundlegenden Satz in der Theorie gewöhnlicher Differentialgleichungen beweisen, welcher sich
ähnlich wie der Beweis des Satzes von Peano an auf Überlegungen von H. Heuser in
\cite{harroheuserGewohnlicheDifferentialgleichungen} beruht. Wir werden in dieser Arbeit die gegebene Beweisidee etwas
genauer ausführen.
\begin{satz}[Existenzsatz- und Eindeutigkeitssatz von Picard-Lindelöf]
\label{picard}
    Seien $t_0 \in \mathbb{R}$, $x_0 \in \mathbb{R}^n$,
    \[
        {\cal G}=\{(t,x) \in \mathbb{R} \times \mathbb{R}^{n}: |t-t_{0}| \leq \alpha, \quad
        \left\lVert x-x_{0} \right\rVert_2 \leq\beta, \quad \alpha,\beta > 0 \},
    \]
    und $f:{\cal G} \rightarrow \mathbb{R}^{n}$ stetig und Lipschitz-stetig im zweiten Argument.
    Dann besitzt das Anfangswertproblem \eqref{firstorder-ode} genau eine Lösung $\hat{x}$ auf dem Intervall
    $D=(t_{0}-a,t_{0}+a)$, wobei
    \[
        a=\min\{\alpha, \frac{\beta}{M}\}, \qquad M= \max_{(t,y)\in {\cal G}}\left\lVert f(t,x)\right\rVert_2 .
    \]\\
\end{satz}
TODO: Induktion korrigieren \\
$Beweis$ Nun ist $f$ Lipschitz-stetig, also setzen wir $B=C(D)$ mit der Maximumsnorm, $U=K$ und $A:K \rightarrow K$
aus dem Beweis von Satz \eqref{peano}. Wir zeigen mit Induktion über $j\in \mathbb{N}$, dass
\[
    \left\lVert (A^j u)(t) - (A^j v)(t) \right\rVert_2 \leq \frac{|t-t_0|^j}{j!} L^j\left\lVert u - v \right\rVert_{\infty},
    \qquad u,v \in K
\]
für alle $j \in \mathbb{N}$ und $t \in D$ gilt. Die Maximumsnorm $\left\lVert \cdot \right\rVert_{\infty}$ wurde
bereits in dem Beweis von Satz \eqref{peano} definiert.
\begin{alignat*}{2}
    (j=1):& \qquad \left\lVert (A u)(t)- (A v)(t) \right\rVert_2 &&\leq
    \underbrace{\left\lVert x_0 - x_0 \right\rVert_2}_{=0}
    + \left\lVert \int_{t_0}^{t} f(s,u(s)) - f(s,v(s)) ds \right\rVert_2\\
    & &&\leq L \left\lVert u - v \right\rVert_{\infty} |t - t_0|\\
    (\text{IV}):& \qquad
    \left\lVert (A^j u)(t) - (A^j v)(t) \right\rVert_2 &&\leq \frac{|t-t_0|^j}{j!} L^j\left\lVert u - v \right\rVert_{\infty} \\
    (j \rightarrow j+1):& \quad \left\lVert (A^{j+1} u)(t)- (A^{j+1} v)(t) \right\rVert_2
    &&\leq \underbrace{\left\lVert x_0 - x_0 \right\rVert_2}_{=0} +
    \left\lVert\int_{t_0}^{t}f(s,(A^ju)(s)) - f(s,(A^ju)(s))ds \right\rVert_2\\
    & &&\leq L |t - t_0| \left\lVert (A^{j}u) - (A^jv) \right\rVert_{\infty} \\
    &  &&= L |t-t_0| \max\limits_{ t \in [t_0 - a,t_0 + a] } \left\lVert (A^ju)(t) - (A^jv)(t) \right\rVert_2 \\
    &  &&\underset{\text{IV}}{\leq} L |t-t_0| \frac{|t-t_0|^j}{j!} L^j\left\lVert u - v \right\rVert_{\infty} \\
    & &&= \frac{|t-t_0|^{j+1}}{(j+1)!} L^{j+1} \left\lVert u - v \right\rVert_{\infty}
\end{alignat*}
Nun folgt direkt
\[
    \left\lVert A^j u - A^j v \right\rVert_{\infty} \leq \frac{(aL)^j}{j!} \left\lVert u - v \right\rVert_{\infty}.
\]
Die Reihe $\sum_{j=1}^{\infty} \frac{(aL)^j}{j!} $ ist nach dem Quotientenkriterium offensichtlich konvergent. Also
lässt sich der Fixpunktsatz von Weissinger \eqref{Fixpunktsatzvonweissinger} anwenden. Der daraus gewonnene Fixpunkt
ist eindeutig und mit ähnlicher Äquivalenz zu dem Beweis von Peano folgt, dass der Fixpunkt die gesuchte eindeutige
Lösung ist. \qedwhite \\
Für lineare Differentialgleichungen erster Ordnung \eqref{linear-first-order-ode} mit einer konstanter Matrix $A$ gilt
wegen
\[
    \left\lVert Ax + g(t) - Ay - g(t) \right\rVert_2 \leq \left\lVert A \right\rVert_2 \left\lVert x - y \right\rVert_2,
\]
dass die rechte Seite Lipschitz-stetig in dem zweiten Argument ist, wodurch mit Satz \eqref{picard} eindeutige
Lösbarkeit folgt. Die Lösung $x$ kann mit Hilfe der \textit{Matrixexponentialfunktion} \cite{Matrixexponential}
angegeben werden:
\begin{align}
    \label{linear-ode-solution}
    x(t) = e^{A(t-t_0)}x_0 + \int_{t_0}^{t}e^{A(t-s)}g(s) ds.
\end{align}
Einfaches Ableiten und Einsetzen von $t_0$ zeigt, dass diese Funktion die Differentialgleichung
\eqref{linear-first-order-ode} tatsächlich löst.

\subsection{Abhängigkeit der Lösungen von den Daten}
In späteren Abschnitten werden wir gewöhnliche Differentialgleichungen betrachten, die zur Simulation/Vorhersage
abgeschlossener Systeme genutzt werden. Darin ist es üblich, dass Anfangsdaten durch Messfehler
von tatsächlichen Daten abweichen. Deshalb ist es sinnvoll Aussagen zu betrachten, die zeigen, welche Auswirkungen
kleine Störungen auf die Lösung der Differentialgleichungen haben. In diesem Abschnitt ist die rechte Seite $f$ stetig
und Lipschitz-stetig im zweiten Argument, sodass wir die Eindeutigkeit der Lösung durch den Satz von Picard-Lindelöff
\eqref{picard} garantieren. Um eine Aussage über die stetige Abhängigkeit von den Daten treffen zu können, beweisen
wir zuerst einen wichtigen Hilfssatz. Die grundsätzlichen Überlegungen und Ergebnisse in diesem Abschnitt stammen aus
\cite{beckGewohnlicheDifferentialgleichungen2018}.
\begin{theorem}[Gronwallsche Ungleichung]
    \label{Satz-gronwall}
    Seien $D=[t_{0}, t_{f}]$ ein Intervall und die stetige, nichtnegative Funktion $u : D \rightarrow \mathbb{R}$
    sowie $a \geq 0, b > 0$ gegeben. Des Weiteren gilt folgende Ungleichung
    \[
        u(t) \leq \alpha \int_{t_{0}}^{t}u(s)ds + \beta
    \]
    für alle $t \in D$. Dann gilt:
    \[
        u(t) \leq e^{\alpha(t-t_{0})}\beta
    \]
    für alle $t \in D$.
\end{theorem}
$Beweis.$ Definiere zuerst eine Hilfsfunktion
\[
    v(t) := \alpha \int_{t_{0}}^{t} u(s)ds + \beta.
\] Für diese gilt
\[
    v^\prime(t) = \alpha u(t) \leq \alpha v(t)
\] für alle $t \in D$. Daraus folgt
\[
    (e^{-\alpha t}v(t))^\prime = e^{-\alpha t}(v^\prime(t)-\alpha v(t)) \leq 0, \qquad t \in D.
\]
Die Funktion $e^{-\alpha t} v(t)$ ist also monoton fallend, das bedeutet
\[
    e^{-\alpha t} u(t) \leq e^{-\alpha t} v(t) \stackrel{t \geq t_{0}}{\leq} e^{-\alpha t_{0}} v(t_{0}) = e^{-\alpha t_{0}}\beta.
\] Daraus folgt die Behauptung. \qedwhite \\
Außerdem benötigen wir noch folgendes Lemma.
\begin{lemma}
    \label{abschaetzungslemma}
    Sei $\mathcal{M} \subset \mathbb{R} \times \mathbb{R}^{n}$ offen und $f:\mathcal{M} \rightarrow \mathbb{R}$ eine
    stetige Funktion, die zusätzlich Lipschitz-stetig im zweiten Argument ist mit
    \[
        \left\lVert f(t, x) - f(t,y) \right\rVert_2 \leq L \left\lVert x - y \right\rVert_2
    \]
    für alle $(t,x),(t,y) \in \mathcal{M}$ mit $L > 0$.
    Ist $\hat{x}$ eine stetig differenzierbare Funktion auf dem Intervall $D \subset \mathbb{R}$ und eine Lösung des
    Anfangswertproblems \eqref{firstorder-ode} und ist $\hat{x}_a$ eine stetig differenzierbare Funktion und eine
    Näherungslösung mit $(t,\hat{x}_a(t))\in \mathcal{M}$ für alle $t \in D$ und es gilt
    \begin{align*}
        \left\lVert \hat{x}_a^{\prime}(t) - f(t,\hat{x}_a(t)) \right\rVert_2 &\leq d_f \qquad t \in D,\\
        |t_{0} - \tilde{t}_0| &\leq d_t,\\
        \left\lVert x_0 - \hat{x}_a(\tilde{t}_0) \right\rVert_2 &\leq d_a\\
    \end{align*}
    ($d_f$ representiert die Störung der rechten Seite, $d_t$ die Störung der Anfangszeit und $d_a$ die Störung
    des Anfangswerts).
    Dann gilt die Abschätzung
    \[
        \left\lVert \hat{x}(t) - \hat{x}_a(t) \right\rVert_2 \leq
        e^{L|t-t_0|}(d_a + d_t(d_f + \sup_{s \in D} \left\lVert f(s, \hat{x}_a(s)) \right\rVert_2)
        + \frac{d_f}{L}) - \frac{d_f}{L}.
    \]
\end{lemma}
$Beweis.$ Betrachte zuerst die Differenz der Lösung $\hat{x}$ und $\hat{x}_a$ bei $t = t_0$:
\begin{align*}
    \left\lVert \hat{x}(t_0) - \hat{x}_a(t_0) \right\rVert_2 &= \left\lVert \hat{x}_0 -
    \int_{\tilde{t}_0}^{t_0} \hat{x}_a^\prime(s)ds - \hat{x}_a(\tilde{t}_{0}) \right\rVert_2 \\
    & \leq \left\lVert \hat{x}_0 - \hat{x}_a(\tilde{t}_0)\right\rVert_2 +
    \left\lVert \int_{\tilde{t}_0}^{t_0} [\hat{x}_a^\prime(s) - f(s, \hat{x}_a(s))] ds \right\rVert_2 +
    \left\lVert \int_{\tilde{t}_0}^{t_0} f(s,\hat{x}_a(s)) ds \right\rVert_2 \\
    & \leq d_a + d_t(d_f + \sup_{s \in D} \left\lVert f(s,\hat{x}_a(s)) \right\rVert_2).
\end{align*}
Nun können wir mit Hilfe der Lipschitz-Eigenschaft der rechten Seite $f$ die Differenz für allgemeines
$t \in D , t > t_0$ abschätzen:
\begin{align*}
    \left\lVert \hat{x}(t) - \hat{x}_a(t) \right\rVert_2 &=
    \left\lVert \hat{x}_0 + \int_{t_0}^{t} f(s,\hat{x})ds - \hat{x}_a(t_0) - \int_{t_0}^{t} \hat{x}_a^{\prime}(s) ds \right\rVert_2\\
    &\leq \left\lVert \hat{x}_0 - \hat{x}_a(t_0) \right\rVert_2 +
    \int_{t_0}^{t} [\left\lVert f(s,\hat{x}(s)) - f(s,\hat{x}_a(s)) \right\rVert_2 +
    \left\lVert \hat{x}_a^{\prime}(s) - f(s,\hat{x}_a(s)) \right\rVert_2] ds \\
    &\leq d_a + d_t(d_f + \sup_{s\in D}\left\lVert f(s,\hat{x}_a(s)) \right\rVert_2) +
    \int_{t_0}^{t} [L \left\lVert \hat{x}(s) - \hat{x}_a(s) \right\rVert_2 + d_f] ds.
\end{align*}
Um das Gronwallsche Lemma verwenden zu können, setzen wir
$u(t):=\left\lVert \hat{x}(t) - \hat{x}_a(t)\right\rVert_2 + \frac{d_f}{L}$,
\[
    \beta:=d_a + d_t(d_g + \sup_{s\in D}\left\lVert f(s,\hat{x}_a(s)) \right\rVert_2) + \frac{d_f}{L})
\] und $\alpha:=L$.
Offensichtlich gilt
\begin{align*}
    &u(t) \leq \alpha \int_{t_0}^{t} u(s) ds + \beta.\\
\end{align*}
Also können wir das Gronwallsche Lemma anwenden und somit folgt
\[
    \left\lVert \hat{x}(t) - \hat{x}_a(t)\right\rVert_2 + \frac{d_g}{L} \leq
    e^{L(t-t_0)}\left[d_a + d_t(d_f + \sup_{s\in D}\left\lVert f(s,\hat{x}_a(s)) \right\rVert_2 + \frac{d_g}{L})\right].
\]
Der Beweis für $t \in D$ mit $t<t_0$ funktioniert analog. \qedwhite\\
Mit diesem Lemma können wir etwas über die stetige Abhängigkeit der Lösung von den Daten aussagen.
\begin{satz}
    \label{Satz-stet-abh}
    Sei $\mathcal{M} \subset \mathbb{R} \times \mathbb{R}^{1+n}$ offen und $f:\mathcal{M} \rightarrow \mathbb{R}^{n}$
    eine stetige Funktion, die Lipschitz-stetig im zweiten Argument mit Konstante $L>0$ gegeben. Dann hängt die
    Lösung $x$ des Anfangswertproblems \eqref{firstorder-ode} stetig von den Anfangsdaten $(t_0, x_0) \in \mathcal{M}$
    und der rechten Seite $f$ ab. Darunter versteht man:
    sind eine Lösung $x$ auf einem kompakten Intervall $D \subset \mathbb{R}$, eine Umgebung U des Graphen
    $\{(t,x(t)): t \in D\}$ in $\mathcal{M}$ und ein $\epsilon>0$ gegeben, dann existiert ein
    $\delta(\epsilon, U, f, D) >0$, sodass die Lösung $\hat{x}$ des gestörten Anfangswertproblems
    \begin{align*}
        \hat{x}^{\prime} &= \hat{f}(t,\hat{x})\\
        \hat{x}(\hat{t}_0) &= \hat{x}_0 \\
    \end{align*}
    auf ganz D existiert und die Abschätzung
    \[
        \left\lVert x(t) - \hat{x}(t) \right\rVert_2 \leq \epsilon \qquad t \in D
    \]
    erfüllt. Voraussetzungen hierfür sind, dass $\hat{t}_0 \in D$, $(\hat{t}_0, \hat{x}_0) \in \mathcal{M}$, $\hat{f}$
    stetig auf U, Lipschitz-stetig im zweiten Argument und
    \[
        |t_0 - \hat{t}_0| \leq \delta, \quad \left\lVert x_0 - \hat{x}_0 \right\rVert_2 \leq \delta, \quad
        \left\lVert f(t,x) - \hat{f}(t,x) \right\rVert_2 \leq \delta, \quad (t,x) \in U
    \] gilt.
\end{satz}
$Beweis.$ Nach Anpassung von $\epsilon$ können wir annehmen, dass $\overline{B_{\epsilon}((t, u(t)))} \subset U$ für
jedes $t \in D$ gilt. Wir definieren $\hat{D}_{\text{max}}$ als das maximale Existenzintervall der Lösung $\hat{x}$ des
gestörten Anfangswertproblems. Nun gilt
\begin{align*}
    \left\lVert \hat{x}^{\prime}(t) - f(t, \hat{x}(t)) \right\rVert_2 =
    \left\lVert f(t,\hat{x}(t)) - \hat{f}(t, \hat{x}(t)) \right\rVert_2 \leq \epsilon
\end{align*}
für alle $t \in \hat{D}_{\text{max}}$. Setzen wir $d_f = d_t = d_a = \delta$, so können wir Lemma
\ref{abschaetzungslemma} anwenden und erhalten
\begin{align*}
    \left\lVert x(t) - \hat{x}(t)\right\rVert_2 \leq e^{L|t-t_0|}(\delta + \delta(\delta +
    \sup\limits_{s \in D}\left\lVert f(s,\hat{x}(s))\right\rVert_2) + \frac{\delta}{L})
\end{align*}
für alle $t \in D \cap \hat{D}_{\text{max}}$. Hierbei ist $\delta > 0$ frei wählbar, weshalb wir $\delta$ in
Abhängigkeit von $\epsilon, \sup\limits_{s \in D}\left\lVert f(s,\hat{x}(s))\right\rVert_2, L$ und $|t-t_0|$ so
bestimmen, dass
\[
    \left\lVert x(t) - \hat{x}(t) \right\rVert_2 \leq \epsilon, \qquad \text{für } t \in D \cap \hat{D}_{\text{max}}
\]
gilt. Die Lösung $\hat{x}$ verlässt die kompakte Menge
\[
    \{(s,y) \in \mathbb{R} \times \mathbb{R}^n: \left\lVert (s,y) - (t,x(t)) \right\rVert_2 \leq \epsilon
    \text{ für ein } t \in D \} \subset \mathcal{M}
\]
nicht. Also können wir mit einem Satz über das Verhalten der maximalen Lösung welcher in
\cite[Seite 60, Satz 3.21]{beckGewohnlicheDifferentialgleichungen2018} gefunden werden kann, gilt
$D \subset \hat{D}_{\text{max}}$. Also haben wir die Fehlerabschätzung $\left\lVert x(t) - \hat{x}(t) \right\rVert_2 \leq \epsilon$
für $t \in D$ und die Existenz der Lösung $\hat{x}$ des gestörten Anfangswertproblems auf $D$ gezeigt. \qedwhite \\
%Um die Abhängigkeit der Anfangsdaten $(t_0,x_0)$ formulieren zu können, wird im Folgenden eine Notation eingeführt.
%\begin{definition}
%    Seien $T \subset \mathbb{R}^{1+n}$ offen und $f:T \rightarrow \mathbb{R}^{n}$ eine stetige Funktion, die
%    Lipschitz-stetig bezüglich der x-Variable ist. Die Abbildung
%    \[
%        (t, t_0, x_0) \mapsto x(t; t_0, x_0)
%    \]
%    mit $(t_0, x_0)\in T$ und $t \in I_{max}(t_0,x_0) = \left( I^{-}(t_0,x_0), I^{+}(t_0,x_0) \right)$ bezüglich der
%    maximalen Lösung des Anfangswertproblems \eqref{firstorder-ode} und dem maximalen Existenzintervall $I_{max}(t_0,x_0)$ heißt
%    charakteristische Funktion der gewöhnlichen Differentialgleichung.
%    Dabei nennt man
%    \[
%        I^{-}(t_0,x_0)=\sup\{ t\in\mathbb{R}: \text{das AWP \eqref{firstorder-ode} ist auf } \left[ t_0, t \right] \text{ lösbar}\}
%    \] und
%    \[
%        I^{+}(t_0,x_0)=\inf\{ t\in\mathbb{R}: \text{das AWP \eqref{firstorder-ode} ist auf } \left[ t, t_0 \right] \text{ lösbar}\}
%    \]
%    die Lebensdauerfunktion.
%\end{definition}
%Satz \ref{Satz-stet-abh} besagt, dass die charakteristische Funktion in allen Argumenten $(t, t_0, x_0)$ stetig ist und
%dass $I^{-}(t_0,x_0)$ bzw. $I^{+}(t_0,x_0)$ ober- bzw. unterhalbstetig sind. Dies bedeutet, dass durch kleine Störungen der
%Anfangswerte $(t_0,x_0)$ sich das maximale Existenzintervall nur stetig verkleinern kann.
%\begin{satz}
%    \label{Satz-diff-abh}
%    Seien $T \subset \mathbb{R}^{1+n}$ und $f:T \rightarrow \mathbb{R}^{n}$ eine stetige Funktion, die in der x-Variable
%    stetig differenzierbar ist. Dann ist die charaktistische Funktion $x(t; t_0, x_0)$ stetig differenzierbar in
%    $(t_0, x_0) \in T$ und $t \in I_{max}(t_0, x_0)$.
%\end{satz}
%$Beweis.$ \cite[69,70]{beckGewohnlicheDifferentialgleichungen2018}
%\begin{bem}
%    Man kann zeigen, dass für $T \subset \mathbb{R}^{1+n}$ und eine stetige Funktion $f:T \rightarrow \mathbb{R}^{n}$,
%    die stetig differenzierbar in der x-Variable ist, gilt: $f$ ist lokal Lipschitz-stetig in der x-Variable.
%    Das bedeutet, dass in Satz \ref{Satz-diff-abh} eine verstärkte Vorraussetzung an die rechte Seite
%    im Gegensatz zu Satz \ref{Satz-stet-abh} verlangt wird.
%\end{bem}
