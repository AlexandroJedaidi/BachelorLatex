\section{neuronale Netze}
In diesem Abschnitt werden wir eine weitere Möglichkeit betrachen, ein Anfangswertproblem zu lösen.
Dabei gehen wir genauer auf neuronale Netze ein. Dieser Abschnitt bezieht sich hauptsächlich auf
\cite{ovidiucalinDeepLearningArchitectures} und auch Grundlagen zu neuronalen Netzen wie Beispielsweise
\textit{Forward}- und \textit{Backwardpropagation} lassen sich in \cite[Chapter 6]{ovidiucalinDeepLearningArchitectures}
nachlesen. Obwohl die bisher besprochenen numerischen Verfahen wie Runge-Kutta- und Mehrschrittverfahren heutzutage
extrem effizient in Bezug auf Fehlertoleranz und Rechenaufwand sind, gibt es dennoch Vorteile neuronale Netze zu
benutzen. Falls Beispielsweise die Interesse besteht, die Lösung einer gewöhnlichen Differentialgleichung nur in einem
bestimmten Zeitpunkt $\hat{t}$ auszuwerten, so müssen in numerischen Verfahren von der Anfangszeit $t_0$ bis zur
gewünschten Zeit $\hat{t}$ iteriert werden. Ein \textit{trainiertes} neuronales Netz repräsentiert jedoch die gesuchte
Lösung und ist deshalb in der Lage eine Funktionsauswertung in $\hat{t}$ auszuführen, was im Gegensatz zu den
numerischen Verfahren viel Rechenarbeit spart. Neuronale Netze haben in unserem Anwendungsgebiet noch einen
ausschlaggebenden Vorteil in Bezug auf Recheneffizienz, welchen wir in Abschnitt \ref{subsec:lsgpakete} betrachten
werden.

\subsection{Struktur neuronaler Netze}
\label{subsec:struktur-eines-neuronalen-netzes}
Zuerst werden wir die grundlegenden Begrifflichkeiten eines neuronalen Netzes definieren. Zur Übersicht beschränken
wir uns auf ein neuronales Netz mit einer Eingangsschicht mit $3$ Neuronen, zwei versteckte Schichten mit jeweils
3 Neuronen und eine Ausgangsschicht mit 2 Neuronen wie in \eqref{neuralNetexample} dargestellt. Die gelben Neuronen
repräsentieren hier jeweils ein \textit{on-Neuron}, oder \textit{bias}, welches konstant $x_0^{(l)} = -1$ ist. Allgemein
haben wir $L$ Schichten und $n^{(l)}$, $1\leq l \leq L$, Neuronen pro Schicht, wobei der bias ausgeschlossen ist.
Die Gewichte $\omega_{ij}^{l}$ geben an, in welcher Relation die Neuronen der vorherigen Schicht $l-1$ zu den Neuronen der
nächsten Schicht $l$ stehen. Dabei werden die Gewichte der Bias-Neuronen mit $b_j^{(l)}:= \omega_{0j}^{(l)}$ von den
anderen Gewichten unterschieden.
\begin{figure}[htp]
    \centering
    \begin{neuralnetwork}[height=4]
        \newcommand{\x}[2]{$x^{(0)}_#2$}
        \newcommand{\y}[2]{$y_#2$}
        \newcommand{\hfirst}[2]{\small $x^{(1)}_#2$}
        \newcommand{\hsecond}[2]{\small $x^{(2)}_#2$}
        \inputlayer[count=3, bias=true, title=Eingabe-\\schicht, text=\x]
        \hiddenlayer[count=3, bias=true, title=versteckte\\Schicht 1, text=\hfirst]
        \linklayers
        \hiddenlayer[count=3, bias=true, title=versteckte\\Schicht 2, text=\hsecond]
        \linklayers
        \outputlayer[count=2, title=Ausgabe-\\schicht, text=\y] \linklayers
    \end{neuralnetwork}
    \caption{neuronales Netzwerk mit 3 Eingabeneuronen (grün), bias (gelb), zwei versteckte Schichten
        (blau) und 2 Ausgabeneuronen (rot)}
    \label{neuralNetexample}
\end{figure}
!! TODO: Frage ob ich das mit Seminararbeit citen muss !!\\
Die Signale der Neuronen ab der ersten versteckten Schicht bilden sich aus der Summe aller vorherigen
Signale mit ensprechender Gewichtung:
\[
    s_j^{(l)} = \sum_{i=1}^{n^{(l)}} \omega_{ij}^{(l)} x_i^{(l-1)} - b_j^{(l)}.
\]
Darauffolgend ergeben sich die Ausgaben der nächsten Schicht $l$
\[
    x_j^{(l)}=\Psi(s_j^{(l)}).
\]
Dabei ist $\Psi: \mathbb{R} \rightarrow \mathbb{R}$ die sogenannte \textit{Aktivierungsfunktion}. Das Trainieren eines
neuronalen Netzes lässt sich im Grunde auf ein Minimierungsproblem einer \textit{Kostenfunktion}
\[
    C: \mathbb{R}^{\left(n^{(l-1)} + 1\right) \times n^{(l)} \times L} \rightarrow \mathbb{R}
\]
reduzieren. Dazu werden die Konzepte der Forward- und Backwardpropagation, sowie ein \textit{Gradientenverfahren}
benötigt, wofür auf \cite[6.2.6]{ovidiucalinDeepLearningArchitectures} verwiesen wird. Zusammengefasst werden die
Gewichte durch ein Gradientenverfahren angepasst, sodass die Kostenfunktion minimiert wird. In unserer Anwendung werden
wir hauptsächlich den \textit{Tangens hyperbolicus}
\[
    tanh(x)=\frac{e^x - e^{-x}}{e^x + e^{-x}}
\]
als Aktivierungsfunktion nutzen. Die Kostenfunktion werden wir in Abschnitt \eqref{subsec:lsgpakete} betrachten. Unser
Fokus der Gradientenverfahren wird auf der \textit{Adam} (engl. Adaptive moment Estimation) liegen. Sei
$g_k := \nabla C(\omega^{[k]})$ der Gradient der Kostenfunktion nach den Gewichten (inklusive bias) in der $k$-ten
Iteration. Dann ist das Adams-Verfahren mit den Variablen $\beta_1,\beta_2 \in [0,1)$ und den Momenten $m_{0}=0$,
$v_{0}=0$
\begin{align*}
    m_{k} &= \beta_1 m_{k-1} + (1-\beta_1)g_k \\
    v_{k} &= \beta_2 v_{k-1} + (1-\beta_2)(g_k)^2
\end{align*}
und deren Korrekturen
\begin{align*}
    \hat{m}_{k} &= \frac{m_{k}}{(1-\beta_1^k)} \\
    \hat{v}_{k} &= \frac{v_{k}}{(1-\beta_2^k)}
\end{align*}
gegeben durch
\[
    x_{k+1}= x_{k} - \eta \frac{\hat{m}_{k}}{\sqrt(|\hat{v}_{k}|) + \epsilon}.
\]
Dabei ist $\eta > 0$ die sogenannte \textit{learning rate} und $\epsilon > 0$ verhindert eine Division durch $0$. Eine
Voreinstellung der Parameter wäre Beispielsweise $\beta_1=0.9$, $\beta_2=0.99$, $\epsilon=10^{-8}$ und $\eta = 0.001$.
Zu Beginn ($k=0$) müssen jedoch schon Gewichte gegeben sein, um $g_k$ berechnen zu können. Dazu werden wir uns in
Abschnitt\eqref{subsec:gewichtsinitialisierung} genauere Gedanken machen.\\
Während des Lernprozesses eines neuronalen Netzes wird über die gesamten Eingangsdaten $X$ iteriert. Dabei nennt man ein
einzelnes Element von $X$ \textit{Probe} (engl. sample). Eine Ansammlung an samples heißt wiederum \textit{Batch}. Eine
Iteration über Batches bewirkt eine erhöhte Effizienz des Lernprozesses
(siehe \cite[6.2.8]{ovidiucalinDeepLearningArchitectures}). Außerdem werden die Lernprozesse auch \textit{Epochen}
genannt, dessen Anzahl angibt, wie oft das neuronale Netz die gegebenen Eingangsdaten zum Training nutzt.

\subsection{Prinzip der Lösungspakete}
\label{subsec:lsgpakete}
Unsere Problemstellung liefert im Allgemeinen keine beschriebene Datenmenge (engl. labeled data), mit dem wir unser
neuronales Netz trainieren können. Wir haben lediglich die gewöhnliche Differentialgleichung erster Ordnung
\begin{align}
    \label{dgl-machinelearnung}
    x^{\prime} = f(t,x)
\end{align}
mit den Anfangsbedingungen $x(t_0)=x_0$ gegeben. Nun werden wir diese parametrisiert durch physikalische Parameter
$\theta$ nach $0$ auflösen. Dabei definieren wir die Funktion
\begin{align}
    \label{g-func}
    &G:\mathbb{R}^n \times C([t_0,t_f],\mathbb{R}) \times [t_0, t_f] \times \mathbb{R}^p \rightarrow \mathbb{R}^n \nonumber\\
    &G \left( x, x^{\prime}, t, \theta \right) = 0,
\end{align}
wobei $p$ die Anzahl der physikalischen Parametern ist. Hierfür werden wir ein sogenanntes \textit{Lösungspaket}
(engl. solution bundle) bilden, welches aus den Lösungen der Gleichung \eqref{g-func} besteht.
\begin{definition}
    \label{sol-bundle}
    Seien die Teilmengen $X_0 \subset \mathbb{R}^n$, $\Theta \subset \mathbb{R}^p$ und $[t_0,t_f] \subset \mathbb{R}$
    gegeben. Dann heißt
    \[
        x(t;x_0, \theta)
    \]
    Lösungspaket der gewöhnlichen Differentialgleichung \eqref{g-func}, wobei $t \in [t_0,t_f]$, $x_0 \in X_0$ und
    $\theta \in \Theta$.
\end{definition}
Unser Ziel ist es diese Lösungspakete mit Hilfe neuronaler Netze zu approximieren. Wir definieren die Approximation der
Lösung über $(X_0,\Theta)$ mit
\begin{align}
    \label{nn-approx}
    \hat{x}(t;x_0, \theta) = x_0 + a(t) N(t; x_0, \theta; \omega),
\end{align}
wobei $N:\mathbb{R}^{1+n+p} \rightarrow \mathbb{R}^n$ das neuronale Netz mit den Gewichten $\omega$ repräsentiert und
$a:[t_0,t_f] \rightarrow \mathbb{R}$ hat die Eigenschaft $a(t_0)=0$. Hierdurch wird das Erfüllen der
Anfangswertbedingungen in der Abschätzung \eqref{nn-approx} sichergestellt. Durch wurde Tests in
\cite{flamantSolvingDifferentialEquations2020} wurde herausgefunden, dass sich für $a$ die Form
\[
    a(t) = 1 - e^{t_0-t}
\]
besonders gut eignet. Zusammengefasst wollen wir für jedes Lösungspaket $(t_i,x_{0i}, \theta_i)$ eine Approximation
$\hat{x}$ finden, so dass
\[
    G(\hat{x}(t_i;x_{0i},\theta_i), \hat{x}^{\prime}(t_i;x_{0i},\theta_i),t_i;\theta_i) \approx 0.
\]
Die Kostenfunktion ergibt sich dann durch
\begin{align}
    \label{cost-func}
    L = \frac{1}{|B|} \sum_{(t_i,x_{0i},\theta_i) \in B} b(t_i)
    \left\lVert G(\hat{x}(t_i;x_{0i},\theta_i), \hat{x}^{\prime}(t_i;x_{0i},\theta_i),t_i;\theta_i) \right\rVert_2^2,
\end{align}
wobei $b:[t_0,t_f] \rightarrow \mathbb{R}$ durch $b(t) = e^{\lambda (t_0 - t)}$ gegeben ist. Die Batches \textit{B}
bestehen aus Ansammlungen von 3-Tupel $(t_i,x_{0i},\theta_i)$, also
$B=\{(t_i,x_{0i},\theta_i) \in [t_0,t_f] \times X_0 \times \Theta\}$ mit der batch size $|B|$. Mit vorgegebenen Gebieten
für $X_0$ und $\Theta$ durch die Problemstellung (bspw. physikalische Zusammenhänge der gegebenen Differentialgleichung)
können die Eingabewerte bestimmt werden. Dafür genügt in den meisten Fällen eine zufällige Wahl aus einer
Gleichverteilung der Intervalle $X_0$ und $\Theta$. In Abschnitt \eqref{subsec:curriculum-learning} werden wir Beispiele
betrachten, in den eine einfache Gleichverteilung zu fehlerhaften Ergebnisse führen würde. Da wir in unserer Anwendung
theoretisch unendlich Eingangsdaten zur Verfügung haben, kann der Begriff einer \textit{Epoche} nicht definiert werden.
Stattdessen repräsentiert jeder Batch eine einzigartige Probe die für den Trainingsablauf genutzt wird, wobei eine
steigende Anzahl der samples eine bessere Approximation liefert. Das Problem des \textit{Overfittings} kann hierbei
nicht auftreten, da es umgangssprachlich nicht möglich ist "unendlich Daten auswendig zu lernen". !!TODO: fragen ob das
geht und cite overfitting!!\\
Der Korrekturfaktor $b(t_i)$ wird zur Beeinflussung des lokalen Fehlers
\begin{align*}
    \tau(t_i;x_{0i}, \theta_i)
    := G \left( \hat{x}(t_i;x_{0i}, \theta_i), \hat{x}^{\prime}(t_i;x_{0i}, \theta_i),t_i;\theta_i \right)
\end{align*}
eingeführt. Ähnlich wie in Satz \eqref{one-step-error-bound} für Einschrittverfahren können wir auch hier einen
Zusammenhang des lokalen und dem globalen Fehlers des neuronalen Netzes zeigen. Dabei ist der globale Fehler des
neuronalen Netzes gegeben mit $e(t) = \hat{x}(t) - x(t)$, wobei $x$ die Lösung von \eqref{dgl-machinelearnung} ist.
\begin{satz}
    Für \eqref{dgl-machinelearnung} ist der lokale Fehler $\tau$ gegeben durch
    $\tau(t) = \hat{x}^{\prime}(t) - f(t,\hat{x})$. Es gilt
    \[
        \left\lVert e(t) \right\rVert = \frac{\tau_{t_f}}{L} \left( e^{L(t-t_0) - 1} \right),
    \]
    wobei $\tau_{t_f} = \max\limits_{t_0 \leq t \leq t_f} \left\lVert \tau(t) \right\rVert$ und $L$ die
    Lipschitz-Konstante der rechten Seite $f$.
\end{satz}
$Beweis.$ Der lokale Fehler ergibt sich durch einfaches Konstuierens von $G$.\\
Für den globalen Fehler gilt
\begin{alignat*}{2}
    &e(t) &= \hat{x}(t) - x(t)\\
    \Leftrightarrow \quad &\hat{x}(t) &= e(t) + x(t).
\end{alignat*}
Einsetzen in den lokalen Fehler ergibt
\begin{alignat*}{2}
    &\tau(t) &= e^{\prime}(t) + x^{\prime}(t) - f(t,e(t)+x(t))\\
    \Leftrightarrow \quad &e^{\prime}(t) &= \tau(t) - x^{\prime}(t) + f(t,e(t) + x(t)).
\end{alignat*}
$x$ die Lösung von \eqref{dgl-machinelearnung}, also ist $x^{\prime}=f(t,x(t))$. Des Weiteren ist $f$ Lipschitz-stetig,
das heißt es gilt $\left\lVert f(t,x(t)+e(t)) - f(t,x(t)) \right\rVert \leq L \left\lVert e(t) \right\rVert$. Also kann
man die Norm der rechten Seite obiger Gleichung schreiben als
\begin{align*}
    \left\lVert \tau(t) + f(t,e(t) + x(t)) - x^{\prime}(t) \right\rVert
    &= \left\lVert \tau(t) + f(t,e(t) + x(t)) - f(t,x(t)) \right\rVert \\
    &\leq \left\lVert \tau(t) \right\rVert + L\left\lVert e(t) \right\rVert \\
    &\leq \tau_{t_f} + L \left\lVert e(t) \right\rVert.
\end{align*}
Dabei wurde die Dreiecksungleichung benutzt und der lokale Fehler wurd durch das Maximum aller lokalen Fehler
abgeschätzt. Jetzt suchen wir eine Funktion $E(t)$ die folgende Gleichung erfüllt
\[
    E^{\prime}(t) = \tau_{t_f} + L E(t),
\]
wobei $0 \leq \left\lVert e(t) \right\rVert \leq E(t)$ und $E(t_0) = 0$ gilt. Also haben wir eine lineare gewöhnliche
Differentialgleichung erster Ordnung welche mit \eqref{linear-ode-solution} berechnen lässt
\begin{align*}
    E(t) &= e^{L(t-t_0)}E(t_0) + \int_{t_0}^{t}e^{L(t-s)}\tau_{t_f} ds \\
    &= \tau_{t_f} e^{Lt} \int_{t_0}^{t}e^{-Ls} ds \\
    &= \frac{\tau_{t_f} }{L} \left( e^{L(t-t_0)} - 1 \right).
\end{align*}
Daraus folgt
\[
    \left\lVert e(t) \right\rVert \leq \frac{\tau_{t_f}}{L} \left( e^{L(t-t_0)} - 1 \right),
\]
was sich mit \eqref{one-step-error-bound} vergleichen lässt. \qedwhite\\
Hierdurch lässt sich sehen, dass der globale Fehler durch einen frühen lokalen Fehler exponentiell mit der Zeit wachsen
kann. Deshalb ist es sinnvoll, die exponentiell fallende Korrekturfunktion $b$ zu nutzen, um den lokalen Fehler in
frühen Zeitpunkten zu skalieren.

\subsection{Gewichtsinitialisierung}
\label{subsec:gewichtsinitialisierung}


\subsection{curriculum learning}
\label{subsec:curriculum-learning}