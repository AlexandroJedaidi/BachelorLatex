\section{Verfahrensvergleich}
\label{sec:verfahrensvergleich}
Wir werden nun die numerischen Verfahren zur Lösung von Anfangswertproblemen mit dem Verfahren der Lösungspakete
vergleichen. Es werden hierfür die Python Bibliotheken \textit{numpy} 1.21.4, \textit{scipy} 1.7.2 für numerische
Verfahren und \textit{tensorflow} 2.7.0 für die Architektur der neuronalen Netze verwendet. Das gegebene
Anfangswertproblem wird zuerst mithilfe des \textit{4-5-Runge-Kutta-Verfahren} beziehungsweise einem BDF-Verfahren
gelöst und danach mit einem neuronalen Netz approximiert. Die folgenden Ergebnisse werden darauffolgend mit
verschiedenen Maßen verglichen. Dabei sind unter anderem die \textit{Trajektorien} der berechneten Lösungen gemeint.
Eine Trajektorie ist hierbei das Bild $T(x_0):= \{x(t,x_0): t\in I_{\text{max}}\}$ der Lösung des Anfangswertproblems
\eqref{first-order-num}, wobei $I_{\text{max}}$ das maximale Existenzintervall ist. Des Weiteren werden wir den globalen
Fehler des numerischen Verfahrens $\left\lVert x(t_i) - u_i \right\rVert_2$ mit dem globalen Fehler der neuronalen Netze
$\left\lVert \hat{x}(t) - x(t) \right\rVert_2$ in Abhängigkeit der Zeit $t_i=t_0 + ih$ und $t\in D$ vergleichen. Dabei
ist zu beachten, dass eine exakte Lösung $x$ vorliegen muss. In dem Abschnitt \ref{subsec:rebound-Pendel} ist das
analytische Berechnen der Lösung aufwendig, weshalb es nicht möglich ist, die globalen Fehler zu vergleichen. Deshalb
werden wir die \textit{Residuen} $0 = u_i^{\prime} - f(t_i,u_i)$ und $0 = u_i^{\prime}- f(t,\hat{x})$
vergleichen, wobei die Ableitung $u_i^{\prime}$ mithilfe der sogenannten \textit{Differenzenquotienten} ermittelt wird.
Der Vorwärtsdifferenzenquotient ist definiert durch
\[
       u_i^{\prime} \approx \frac{u_{i+1}-u_i}{h_i}.
\]
Diese Definition findet sich in \cite[177]{peterdeuflhardfolkmarbornemannNumerischeMathematikGewohnliche} wieder.
Zuletzt geben wir die Kostenfunktion der neuronalen Netze in Abhängigkeit der Epochen und, wenn möglich, den globalen
Fehler für alle Zeitwerte in Abhängigkeit der Epochen an. Für das 4-5-Runge-Kutta-Verfahren verwenden wir die Bibliothek
$scipy.integrate$, genauer die Funktion $scipy.integrate.RK45$ (Dokumentation siehe \cite{ScipyIntegrateRK45}). Die
Berechnungen wurden mit einem AMD Ryzen 7 5800X CPU, 32GB RAM und einer NVIDIA RTX 3060 Ti GPU durchgeführt. Der
Quellcode zur Lösung der folgenden Anfangswertproblemen mit numerischen Verfahren und die Implementierung der neuronalen
Netze befindet sich in dem beigelegtem Datenträger.

\subsection{Rebound-Pendel}
\label{subsec:rebound-Pendel}
Ein Rebound-Pendel ist ein System, in dem ein Pendel an einer Seite auf eine gedämpfte Feder treffen kann.
\begin{figure}
       \centering
       \includegraphics{rebound_pendulum_diagram}
       \caption{Diagramm eines Rebound Pendels\cite[6]{flamantSolvingDifferentialEquations2020}}
       \label{fig:rebound_pendulum_diagram}
\end{figure}
Ein Beispiel des beschriebenen Systems wird in Abbildung \ref{fig:rebound_pendulum_diagram} gezeigt. Das zugehörige
Anfangsproblem 2. Ordnung hat die Form
\begin{align*}
       \theta^{\prime \prime} &= - \frac{g}{l} \sin(\theta) + H(-\theta)
       \text{ReLU}(-\frac{kl}{m}\theta - c \theta^{\prime})\\
       \theta(0) &= 1, \quad \theta^{\prime}(0)=0.2,
\end{align*}
beziehungsweise mit der Einführung einer weiteren Variable $\varphi=\theta^{\prime}$ zu einem System erster Ordnung
\begin{align}
       \theta^{\prime} &= \varphi \nonumber \\
       \varphi^{\prime} &= - \frac{g}{l} \sin(\theta) + H(-\theta)
       \text{ReLU}(-\frac{kl}{m}\theta - c \varphi) \label{rebound-pendulum}\\
       \theta(0) &= 1, \quad \varphi(0)=0.2. \nonumber
\end{align}
Hierbei ist $\text{ReLU}(x)= \max(x, 0)$,
\begin{align*}
       H(x) =
       \begin{cases}
              0, &x<0 \\
              1, &x \geq 0
       \end{cases},
\end{align*}
$g$ die Erdbeschleunigung, $l$ die Länge des Pendels, $m$ die Masse des Pendels, $k$ die Federkonstante und $c$ der
Dämpfungskoeffizient. Wir werden dieses System nun in dem Zeitraum $t \in [0, 10]$ lösen bzw. approximieren und die
Resultate vergleichen. Für beide Verfahren werden zur Vereinfachung $g=1$ und $l=1$ gesetzt. Außerdem gilt für das
Runge-Kutta-Verfahren $k=3$ und $c=1$.
\begin{table}
       \renewcommand{\arraystretch}{1.0}
       \centering
       \begin{tabular}{ l | l }
              \hline
              Netzwerkstruktur & \\
              \hline
              Anzahl der Schichten & $L=10$ \\
              Eingabeschicht & $n^{(0)}=5$ mit $\Phi(x)=\tanh(x)$ \\
              versteckte Schichten & $n^{(l)}=128$, $l = 1, \dots, L-1$ mit $\Phi(x)=\tanh(x)$ \\
              Ausgabeschicht & $n^{(L)}=2$ mit $\Phi(x)=x$ \\
              \hline
              Hyperparameter und Initialisierungsintervalle & \\
              \hline
              Anfangswertbereiche & $(\theta_0, \varphi_0) \in [0.0, 1.0] \times [-0.2, 0.2]$ \\
              Paramaterbereiche & $(k, c) \in [2.0, 5.0] \times [0.0, 2.0]$ \\
              Zeitraum & $t \in [0, 10]$ \\
              \hline
              Optimierung & \\
              \hline
              Gradientenverfahren & Adam \\
              Gewichtsfunktion & $b(t)=e^{-0.5t}$ \\
              Batchsize & $|B|=10000$ \\
              Epochen & $1000000$ \\
              Lernrate & $\eta= 0.0001$ \\
              Gewichtsinitialisierung & Xavier-Initialisierung \\
              \hline
              Statistiken & \\
              \hline
              Trainingrate & xx \\
              Trainingszeit & xx Stunden \\
              \hline
       \end{tabular}
       \caption{Netzwerkstruktur, Hyperparameter, Initialisierungsintervalle, Optimierungsparameter und Statistiken
       für das Rebound-Pendel-Anfangswertproblem.}
\label{rebound-pendulum-table}
\end{table}
In Tabelle \ref{rebound-pendulum-table} sind alle relevanten Daten für das Rebound-Pendel-Anfangswertproblem gegeben,
wobei $\Phi(x)$ die Aktivierungsfunktion für die jeweiligen Schichten ist. Wie bereits erwähnt, haben wir keine exakte
Lösung vorliegen, weshalb wir zum Vergleich die Trajektorieren der berechneten Lösungen und den Resdiuumsfehler benutzen.
In Abbildung \ref{fig:rebound-trajectories} sehen wir, dass sich die beiden Trajektorien einen ähnlichen Verlauf haben.
Jedoch liefert dies keine Aussage über die Güte der Ergebnisse, da wir keine Trajektorie der exakten Lösung verwenden
können. Ähnliches gilt für die Abbildung \ref{fig:rebound-trajectories-in-time}. Deshalb betrachten wir
!!TODO: Graphiken und atol,rtol!!

\begin{figure}
       \centering
       \includegraphics[scale=0.9]{Rebound_plots/reboundpendulumtrajectories_}
       \caption{Trajektorie der verschiedenen Lösungen.}
       \label{fig:rebound-trajectories}
\end{figure}

\begin{figure}
       \centering
       \includegraphics[scale=0.9]{Rebound_plots/reboundpendulumtrajectories_in_time_}
       \caption{$(t, \theta(t))$ Graph der berechneten Lösungen.}
       \label{fig:rebound-trajectories-in-time}
\end{figure}

\begin{figure}
       \centering
       \includegraphics[scale=0.9]{Rebound_plots/reboundpendulumlocal_error_}
       \caption{Graph des Residuumfehlers beider Verfahren.}
       \label{fig:rebound-residuum}
\end{figure}

\begin{figure}
       \centering
       \includegraphics[scale=0.9]{Rebound_plots/reboundpendulumML_Loss_}
       \caption{Kostenfunktion des neuronalen Netzes bei steigender Epochenanzahl.}
       \label{fig:rebound-loss}
\end{figure}

\subsection{Steife Differentialgleichung}
\label{sec:steife-differentialgleichung}
Die Anfangswertproblem
\begin{align}
       \label{stiff}
       x_{1}^{\prime} &= \frac{1}{2} ((\lambda_1 + \lambda_2)x_1 + (\lambda_1 - \lambda_2)x_2), \nonumber \\
       x_{2}^{\prime} &= \frac{1}{2} ((\lambda_1 - \lambda_2)x_1 + (\lambda_1 + \lambda_2)x_2), \\
       x_1(0) &= c_1 + c_2, \quad x_2(0) = c_1 - c_2 \nonumber
\end{align}
lässt sich mit der Matrix
\[
       A = \frac{1}{2}
       \begin{bmatrix}
                \lambda_1 + \lambda_2 & \lambda_1 - \lambda_2 \\
                \lambda_1 - \lambda_2 & \lambda_1 + \lambda_2
       \end{bmatrix}
\]
zu
\begin{align*}
       x^{\prime} &= Ax, \\
       x(0) &=
       \begin{bmatrix}
              c_1 + c_2 \\
              c_1 - c_2
       \end{bmatrix}
\end{align*}
umformulieren. Dabei gilt $\lambda_1, \lambda_2 < 0$ und $c_1, c_2 \in \mathbb{R}$. A hat die Eigenwerte
$\lambda_1(A) = \lambda_1$ und $\lambda_2(A) = \lambda_2$ und für $\lambda_1 = -100$ und $\lambda_2 = -1$ ist
das Anfangswertproblem \eqref{stiff} nach Definition \eqref{steife-dgl} $steif$. Da \eqref{stiff} eine lineare
Differentialgleichung erster Ordnung ist, können wir die Lösung mithilfe \eqref{linear-ode-solution} angeben:
\[
       x(t) =
       \begin{bmatrix}
              c_1 e^{\lambda_1 t} + c_2 e^{\lambda_2 t} \\
              c_1 e^{\lambda_1 t} - c_2 e^{\lambda_2 t}
       \end{bmatrix}.
\]
Der erste Eigenwert $\lambda_1$ lässt den ersten Summand der Lösung viel schneller gegen $0$ konvergieren für
$t \rightarrow \infty$, als der zweite Eigenwert $\lambda_2$. Verwenden wir nun das explizite Euler-Verfahren mit der
Form
\begin{align*}
       u_0 &= x_0 \\
       u_i &= u_{i-1} + hAu_{i-1}= \dots = (I + hA)^{i}u_0, \qquad i=1,\dots,N
\end{align*}
um das Anfangswertproblem zu lösen. Die numerische Lösung ist also gegeben mit
\begin{align*}
       u_{i}=
       \begin{bmatrix}
              c_1 (1+h\lambda_1)^{i} + c_2 (1+h\lambda_2)^{i}\\
              c_1 (1+h\lambda_1)^{i} + c_2 (1+h\lambda_2)^{i}
       \end{bmatrix}.
\end{align*}
Damit die exakte Lösung und die numerische Lösung das gleiche Grenzverhalten besitzen, muss $|1 + h\lambda_1|<1$ und
$|1 + h\lambda_2|<1$ gelten. Hieraus resultieren Bedinungen für $h$, also eine Schrittweitenbeschränkung der Form
$h<\frac{2}{|\lambda_1}$ und $h<\frac{2}{\lambda_2}$. Da $|\lambda_2| \ll |\lambda_1|$, bestimmt der erste Eigenwert
über die Beschränkung der Schrittweite, obwohl dieser in der exakten Lösung für größere Zeiten $t$ fast keinen Einfluss
hat. Wir werden sehen, dass dieses Problem auch für Runge-Kutta-Verfahren auftreten, weshalb wir außerdem ein
\textit{BDF-Verfahren}, also ein Mehrschrittverfahren, verwenden werden, um die Ergebnisse mit dem Verfahren der
Lösungspakete vergleich zu können. Für alle Verfahren setzen wir $c_1=3$, $c_2=4$, $\lambda_1 = -100$ und $\lambda_2=-1$.
Für die Implementierung des BDF-Verfahrens wird die Funktion $scipy.integrate.BDF$ (Dokumentation siehe
\cite{ScipyIntegrateBDF}) verwendet. Für beide Verfahren setzen wir dazu die relative Toleranz $rtol=10^{-2}$ und die
absolute Toleranz $atol=10^{-7}$ um die Steiffheit des gegebenen Anfangswertproblems \eqref{stiff} beobachten zu können.
Ähnlich wie in Abschnitt \ref{subsec:rebound-Pendel} sind in Tabelle \ref{stiff-table} alle relevanten Daten zu dem
verwendeten neuronalen Netz \eqref{stiff} gegeben.
\begin{table}
       \renewcommand{\arraystretch}{1.0}
       \centering
       \begin{tabular}{ l | l }
              \hline
              Netzwerkstruktur & \\
              \hline
              Anzahl der Schichten & $L=10$ \\
              Eingabeschicht & $n^{(0)}=5$ mit $\Phi(x)=\tanh(x)$ \\
              versteckte Schichten & $n^{(l)}=32$, $l = 1, \dots, L-1$ mit $\Phi(x)=\tanh(x)$ \\
              Ausgabeschicht & $n^{(L)}=2$ mit $\Phi(x)=x$ \\
              \hline
              Hyperparameter und Initialisierungsintervalle & \\
              \hline
              Anfangswertbereiche &
              $x_{1,0} \in [c_1+c_2 - 0.05, c_1+c_2 + 0.05]$ \\
              & $x_{2,0} \in [c_1-c_2 - 0.05, c_1-c_2 + 0.05]$ \\
              Paramaterbereiche &
              $\lambda_1 \in [\lambda_1 - 0.05, \lambda_1 + 0.05]$ \\
              & $\lambda_2 \in[\lambda_2 - 0.05, \lambda_2 + 0.05]$ \\
              Zeitraum & $t \in [0, 10]$ \\
              \hline
              Optimierung & \\
              \hline
              Gradientenverfahren & Adam \\
%              Gewichtsfunktion & $b(t)=e^{-0.5t}$ \\
              Gewichtsfunktion & $b(t)=1$ \\
              Batchsize & $|B|=10000$ \\
              Epochen & $1000000$ \\
              Lernrate & $\eta= 0.0001$ \\
              Gewichtsinitialisierung & Xavier-Initialisierung \\
              \hline
              Statistiken & \\
              \hline
              Trainingrate & 125 Batches/Sekunde \\
              Trainingszeit & 2.22 Stunden \\
              \hline
       \end{tabular}
       \caption{Netzwerkstruktur, Hyperparameter, Initialisierungsintervalle, Optimierungsparameter und Statistiken
       für das steife Anfangswertproblem \eqref{stiff}.}
       \label{stiff-table}
\end{table}
!!TODO: Grafiken!!
In Abbildung \ref{fig:stiff-trajectories} können wir den Verlauf der Trajektorie mit dem Anfangspunkt
$x_0=x(0)=(7,-1)^{\intercal}$ für die erwähnten Verfahren betrachten. Wie bereits besprochen beschränkt der betragsmäßig große
Eigenwert $\lambda_1=-100$ die Schrittweite $h$, was zu einer ungenauen Lösung des Runge-Kutta-Verfahren führt. Die
Approximation des neuronalen Netzes jedoch lässt sich mit der Lösung des BDF-Verfahrens vergleichen, da es für ein
gewissen Zeitintervall sogar einen niedrigeren globalen Fehler hat als die Lösung des BDF-Verfahrens.
Dieses Verhalten lässt sich auch in der Abbildung des Fehlers in Abhängigkeit der Zeit \ref{fig:stiff-error-in-time}
wiederfinden.
\begin{figure}
       \centering
       \includegraphics[scale=0.9]{Stiff_plots/stifftrajectories_}
       \caption{Trajektorien der verschiedenen Lösungen.}
       \label{fig:stiff-trajectories}
\end{figure}
\begin{figure}
       \centering
       \includegraphics[scale=0.9]{Stiff_plots/stiffError_in_time}
       \caption{Globaler Fehler  der jeweiligen Verfahren in Abhängigkeit der Zeit.}
       \label{fig:stiff-error-in-time}
\end{figure}
Der oszillierende Fehler des Runge-Kutta-Verfahrens stimmt mit dem Verhalten der Trajektorie überein. Des Weiteren fällt
auf, dass der Fehler des neuronalen Netzes über die Zeit nahezu konstant bleibt, während der Fehler der anderen
Verfahren im Laufe der Zeit sinkt. Zudem sehen wir in \ref{fig:stiff-error} und \ref{fig:stiff-loss} den Verlauf des
globalen Fehlers und der Kostenfunktion für steigende Epochen.
\begin{figure}
       \centering
       \includegraphics[scale=0.9]{Stiff_plots/stiffML_error_}
       \caption{Globaler Fehler für steigende Epochen.}
       \label{fig:stiff-error}
\end{figure}
\begin{figure}
       \centering
       \includegraphics[scale=0.9]{Stiff_plots/stiffML_Loss_}
       \caption{Kostenfunktion für steigende Epochen.}
       \label{fig:stiff-loss}
\end{figure}
Sowohl der globale Fehler als auch die Kostenfunktion sinken wie erwartet bei steigender Epchenzahl, jedoch wird die
Steigung beider Graphen betragsmäßig kleiner. Um eine genauere Approximation zu erhalten, müssten wir die Anzahl der
Epochen erhöhen, wobei dies zu einer Laufzeit von über $\approx 2.2$ Stunden führen würde. Das BDF-Verfahren hat
trotz einer Beschränkung der globalen und relativen Toleranz das Anfangswertproblem in $0.039$ Sekunden mit akzeptablen
Fehler gelöst. Deshalb können wir zusammenfassend sagen, dass das BDF-Verfahren allgemein qualifizierter ist zur Lösung
von steifen Anfangswertproblemen.

\subsection{Harmonischer Oszillator}
\label{sec:harmonischer-oszillator}
Zuletzt werden wir die Lösung für den harmonischen Oszillator mit verschiedenen Netzwerkstrukturen approximieren und
die Ergebnisse untereinander und mit dem 4-5-Runge-Kutta-Verfahren vergleichen. Die Bewegung eines harmonischen
Oszillators ist gegeben mit
\begin{align}
       \label{harmonic-oscillator}
       &x^{\prime}=v, \qquad v^{\prime}=-\frac{k}{m}x, \\
       &x(0)=0, \quad v(0)=0, \nonumber
\end{align}
wobei zur Vereinfachung $m=1$ gesetzt wird. Außerdem gilt für das Runge-Kutta-Verfahren $k=1$, für relative Toleranz
$rtol = 10^{-10}$ und die globale Toleranz $atol = 10^{-14}$. Die Tabelle \ref{stiff-table-data} enthält alle Daten,
welche für alle folgenden neuronale Netze verwendet werden.
\begin{table}
       \renewcommand{\arraystretch}{1.0}
       \centering
       \begin{tabular}{ l | l }
              \hline
              Hyperparameter und Initialisierungsintervalle & \\
              \hline
              Anfangswertbereiche &
              $(x_{0},v_0) \in [-1.0, 1.0] \times [-1.0, 1.0]$ \\
              Paramaterbereiche & $k \in [0.5, 2.0]$ \\
              Zeitraum & $t \in [0, 2\pi]$ \\
              \hline
              Optimierung & \\
              \hline
              Gradientenverfahren & Adam \\
              Gewichtsfunktion & $b(t)=1$ \\
              Batchsize & $|B|=10000$ \\
              Epochen & $100000$ \\
              Lernrate & $0.0001$ \\
              Gewichtsinitialisierung & Xavier-Initialisierung \\
              \hline
       \end{tabular}
       \caption{Hyperparameter, Initialisierungsintervalle und Optimierungsparameter für den harmonischen Oszillator
       \eqref{harmonic-oscillator}.}
       \label{stiff-table-data}
\end{table}
Wir werden zuerst eine konstante Anzahl Schichten $L=6$ verwenden und die Anzahl der Neuronen pro Schicht variieren.
Dazu betrachten wir versteckte Schichten mit den Längen
$n^{(l)} = 4,$ $8,$ $16,$ $32$, $l = 1, \dots, L-1$. Tabelle \ref{stiff-table-first} enthält die genauen
Netzwerkstrukturen.
\begin{table}
       \renewcommand{\arraystretch}{1.0}
       \centering
       \begin{tabular}{ l | l }
              \hline
              Netzwerk 1 & \\
              \hline
              Anzahl der Schichten & $L=6$ \\
              Eingabeschicht & $n^{(0)}=4$ mit $\Phi(x)=\tanh(x)$ \\
              versteckte Schichten & $n^{(l)}=4$, $l = 1, \dots, L-1$ mit $\Phi(x)=\tanh(x)$ \\
              Ausgabeschicht & $n^{(L)}=2$ mit $\Phi(x)=x$ \\
              Lernrate & $\eta=0.0001$ \\
              Trainingrate & 195 Batches/Sekunde \\
              Traningszeit & 0.14 Stunden \\
              \hline
              Netzwerk 2 & \\
              \hline
              Anzahl der Schichten & $L=6$ \\
              Eingabeschicht & $n^{(0)}=4$ mit $\Phi(x)=\tanh(x)$ \\
              versteckte Schichten & $n^{(l)}=8$, $l = 1, \dots, L-1$ mit $\Phi(x)=\tanh(x)$ \\
              Ausgabeschicht & $n^{(L)}=2$ mit $\Phi(x)=x$ \\
              Lernrate & $\eta=0.0001$ \\
              Trainingrate & 195 Batches/Sekunde \\
              Trainingszeit & 0.14 Stunden \\
              \hline
              Netzwerk 3 & \\
              \hline
              Anzahl der Schichten & $L=6$ \\
              Eingabeschicht & $n^{(0)}=4$ mit $\Phi(x)=\tanh(x)$ \\
              versteckte Schichten & $n^{(l)}=16$, $l = 1, \dots, L-1$ mit $\Phi(x)=\tanh(x)$ \\
              Ausgabeschicht & $n^{(L)}=2$ mit $\Phi(x)=x$ \\
              Lernrate & $\eta=0.0001$ \\
              Trainingrate & 195 Batches/Sekunde \\
              Trainingszeit & 0.14 Stunden \\
              \hline
              Netzwerk 4 & \\
              \hline
              Anzahl der Schichten & $L=6$ \\
              Eingabeschicht & $n^{(0)}=4$ mit $\Phi(x)=\tanh(x)$ \\
              versteckte Schichten & $n^{(l)}=32$, $l = 1, \dots, L-1$ mit $\Phi(x)=\tanh(x)$ \\
              Ausgabeschicht & $n^{(L)}=2$ mit $\Phi(x)=x$ \\
              Lernrate & $\eta=0.0001$ \\
              Trainingrate & 180 Batches/Sekunde \\
              Trainingszeit & 0.15 Stunden \\
              \hline
       \end{tabular}
       \caption{Netzwerkstrukuren der ersten Variation.}
       \label{stiff-table-first}
\end{table}
!TODO:Bilder!
In Abbildung \ref{fig:harmonic-neurons-variable-loss} und Abbildung \ref{fig:harmonic-neurons-variable-error} lässt
sich leicht erkennen, dass das bei steigender Anzahl der Neuronen sowohl die Kostenfunktion als auch der globale Fehler
das größte Minimum erreicht. Auch in der Abbildung der Trajektorien \ref{fig:harmonic-neurons-variable-trajectories}
sehen wir, dass das neuronale Netz mit den meisten Neuronen pro Schicht die beste Lösung liefert. Dei Lösung des
Runge-Kutta-Verfahrens approximiert die exakte Lösung jedoch viel besser, da die Trajektorie direkt unter der
Trajektorie der exakten Lösung liegt. Des Weiteren sieht man am globalen Fehler des 4. Netzwerkes
\ref{fig:harmonic-neurons-variable-error}, dass dieser die maximale Größenordnung $10^0$ besitzt.
\begin{figure}
       \centering
       \includegraphics[scale=0.9]{harmonicoscillator_plots/harmonicoscillatorML_error__neurons_var_loss}
       \caption{Kostenfunktion der verschiedenen neuronalen Netze.}
       \label{fig:harmonic-neurons-variable-loss}
\end{figure}
\begin{figure}
       \centering
       \includegraphics[scale=0.9]{harmonicoscillator_plots/harmonicoscillatorML_error__neurons_var_error}
       \caption{Globaler Fehler der verschiedenen neuronalen Netze.}
       \label{fig:harmonic-neurons-variable-error}
\end{figure}
\begin{figure}
       \centering
       \includegraphics[scale=0.9]{harmonicoscillator_plots/harmonicoscillator_neurons_vartrajectories}
       \caption{Trajektorien der von den verschiedenen neuronalen Netzen approximierte Lösung, der Runge-Kutta-Lösung
       und der exakten Lösung.}
       \label{fig:harmonic-neurons-variable-trajectories}
\end{figure}
\begin{figure}
       \centering
       \includegraphics[scale=0.9]{harmonicoscillator_plots/harmonicoscillator_neurons_vartrajectories_in_time_}
       \caption{$(t,x(t))$ Graph der Lösungen des Runge-Kutta-Verfahrens, der Approximation der
       Lösungspakete und der exakten Lösung.}
       \label{fig:harmonic-neurons-variable-trajectories-in-time}
\end{figure}

Zuletzt werden wir Netzwerke mit 4, 8 und 16 versteckte Schichten vergleichen. Dazu wird die Länge der jeweiligen
Schichten$n^{(l)}=32$. Genauere Angaben zu den Netzwerken befinden sich in Tabelle \ref{stiff-table-second}.
\begin{table}
       \renewcommand{\arraystretch}{1.0}
       \centering
       \begin{tabular}{ l | l }
              \hline
              Netzwerk 1 & \\
              \hline
              Anzahl der Schichten & $L=6$ \\
              Eingabeschicht & $n^{(0)}=4$ mit $\Phi(x)=\tanh(x)$ \\
              versteckte Schichten & $n^{(l)}=32$, $l = 1, \dots, L-1$ mit $\Phi(x)=\tanh(x)$ \\
              Ausgabeschicht & $n^{(L)}=2$ mit $\Phi(x)=x$ \\
              Lernrate & $\eta=0.0001$ \\
              Trainingrate & 185 Batches/Sekunde \\
              Traningszeit & 0.15 Stunden \\
              \hline
              Netzwerk 2 & \\
              \hline
              Anzahl der Schichten & $L=10$ \\
              Eingabeschicht & $n^{(0)}=4$ mit $\Phi(x)=\tanh(x)$ \\
              versteckte Schichten & $n^{(l)}=32$, $l = 1, \dots, L-1$ mit $\Phi(x)=\tanh(x)$ \\
              Ausgabeschicht & $n^{(L)}=2$ mit $\Phi(x)=x$ \\
              Lernrate & $\eta=0.0001$ \\
              Trainingrate & 129 Batches/Sekunde \\
              Trainingszeit & 0.22 Stunden \\
              \hline
              Netzwerk 3 & \\
              \hline
              Anzahl der Schichten & $L=18$ \\
              Eingabeschicht & $n^{(0)}=4$ mit $\Phi(x)=\tanh(x)$ \\
              versteckte Schichten & $n^{(l)}=32$, $l = 1, \dots, L-1$ mit $\Phi(x)=\tanh(x)$ \\
              Ausgabeschicht & $n^{(L)}=2$ mit $\Phi(x)=x$ \\
              Lernrate & $\eta=0.0001$ \\
              Trainingrate & 76 Batches/Sekunde \\
              Traningszeit & 0.36 Stunden \\
              \hline
       \end{tabular}
       \caption{Netzwerkstrukuren der zweiten Variation.}
       \label{stiff-table-second}
\end{table}
Hier fällt auf, dass Netzwerk 1 die niedrigste Kostenfunktion erreicht, was in Abbildung
\ref{fig:harmonic-layers-variable-loss} zu sehen ist. In Abbildung \ref{fig:harmonic-layers-variable-error} lässt sich
jedoch erkennen, dass Netzwerk 2 den niedrigsten globalen Fehler erzielt. Ähnlich wie bei der ersten Variation der
Neuronen befindet sich der globale Fehler des besten Netzwerkes in der Größenordnung $10^0$. Die Trajektorien in
Abbildung \ref{fig:harmonic-layers-variable-trajectories} bestätigen, dass Netzwerk 3 die schlechteste Approximation des
Anfangswertproblems \eqref{harmonic-oscillator} liefert. Außerdem lässt sich an den Trajektorien und Abbildung
\ref{fig:harmonic-layers-variable-trajectories-in-time} erkennen, dass Netzwerk 2 die beste Approximation liefert,
wobei die Trajektorien und auch der $(t_i,u_i)$-Graph der Lösung $u_i$ des Runge-Kutta-Verfahrens wiederum direkt unter
der exakten Lösung liegen.
\begin{figure}
       \centering
       \includegraphics[scale=0.9]{harmonicoscillator_plots/harmonicoscillatorML_error__layers_var_loss}
       \caption{Kostenfunktion der verschiedenen neuronalen Netze.}
       \label{fig:harmonic-layers-variable-loss}
\end{figure}
\begin{figure}
       \centering
       \includegraphics[scale=0.9]{harmonicoscillator_plots/harmonicoscillatorML_error__layers_var_error}
       \caption{Globaler Fehler der verschiedenen neuronalen Netze.}
       \label{fig:harmonic-layers-variable-error}
\end{figure}
\begin{figure}
       \centering
       \includegraphics[scale=0.9]{harmonicoscillator_plots/harmonicoscillator_layers_vartrajectories}
       \caption{Trajektorien der von den verschiedenen neuronalen Netzen approximierte Lösung, der Runge-Kutta-Lösung
       und der exakten Lösung.}
       \label{fig:harmonic-layers-variable-trajectories}
\end{figure}
\begin{figure}
       \centering
       \includegraphics[scale=0.9]{harmonicoscillator_plots/harmonicoscillator_layers_vartrajectories_in_time_}
       \caption{$(t,x(t))$-Graph der von den verschiedenen neuronalen Netzen approximierte Lösung, der
       Runge-Kutta-Lösung und der exakten Lösung.}
       \label{fig:harmonic-layers-variable-trajectories-in-time}
\end{figure}
Zusammenfassend können wir sagen, dass eine Erhöhung der Neuronen und Schichten abhängig von der Komplexität des
Anfangswertproblems ist. Außerdem muss die Trainingszeit erhöht werden, um eine vergleichbar gute Approximation
gegenüber dem Runge-Kutta-Verfahren zu erhalten.
\newpage
